{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d714311",
   "metadata": {},
   "source": [
    "# Linear Regression 03 — Multiple & Polynomial Regression  \n",
    "**Deccan AI School (Premium Bootcamp)** — Working Professionals (IT/Software)\n",
    "\n",
    "**Goal:** Move from toy “one-feature” regression to realistic modeling:\n",
    "- Multiple Linear Regression (many features)\n",
    "- Feature engineering (interaction terms)\n",
    "- Polynomial Regression (non-linear patterns with linear model in transformed space)\n",
    "- Overfitting intuition + train/test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483df75",
   "metadata": {},
   "source": [
    "## 1) A realistic IT dataset (synthetic, but business-shaped)\n",
    "\n",
    "We simulate \"project delivery time\" in days.\n",
    "\n",
    "Features:\n",
    "- story_points (work size)\n",
    "- team_size\n",
    "- tech_complexity (1-10)\n",
    "- dependencies_count\n",
    "\n",
    "Target:\n",
    "- delivery_days\n",
    "\n",
    "This is a very common scenario in IT planning and project management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "n = 500\n",
    "\n",
    "story_points = rng.integers(10, 200, size=n)\n",
    "team_size = rng.integers(2, 15, size=n)\n",
    "tech_complexity = rng.integers(1, 11, size=n)  # 1..10\n",
    "dependencies = rng.integers(0, 12, size=n)\n",
    "\n",
    "# True underlying function (unknown to model)\n",
    "noise = rng.normal(0, 7, size=n)\n",
    "\n",
    "delivery_days = (\n",
    "    0.35 * story_points\n",
    "    - 2.8 * team_size\n",
    "    + 4.5 * tech_complexity\n",
    "    + 1.8 * dependencies\n",
    "    + 0.02 * (story_points * tech_complexity)  # interaction effect\n",
    "    + noise\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"story_points\": story_points,\n",
    "    \"team_size\": team_size,\n",
    "    \"tech_complexity\": tech_complexity,\n",
    "    \"dependencies\": dependencies,\n",
    "    \"delivery_days\": delivery_days\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f3018",
   "metadata": {},
   "source": [
    "## 2) Train/Test split (professional habit)\n",
    "\n",
    "Always split before experimenting, otherwise you will accidentally overfit.\n",
    "\n",
    "In production, you may also do:\n",
    "- validation set\n",
    "- cross-validation\n",
    "But here train/test is enough to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"delivery_days\"])\n",
    "y = df[\"delivery_days\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_train = lr.predict(X_train)\n",
    "pred_test = lr.predict(X_test)\n",
    "\n",
    "def report(y_true, y_pred, name=\"\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mse**0.5\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} MSE={mse:.2f} RMSE={rmse:.2f} MAE={mae:.2f} R2={r2:.3f}\")\n",
    "\n",
    "report(y_train, pred_train, \"Train\")\n",
    "report(y_test, pred_test, \"Test \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c7830",
   "metadata": {},
   "source": [
    "## 3) Interpret coefficients like a business analyst\n",
    "\n",
    "Coefficient meaning:\n",
    "- Holding other features constant, 1 unit increase in feature changes target by coef units.\n",
    "\n",
    "In this IT dataset:\n",
    "- team_size coefficient should be negative (bigger team → faster delivery), but careful:\n",
    "  - In real life, too big team can slow down due to communication overhead.\n",
    "  - Linear regression captures only the linear part unless we add features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coef\": lr.coef_\n",
    "}).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe920a",
   "metadata": {},
   "source": [
    "## 4) Feature engineering: Interaction terms\n",
    "\n",
    "We secretly built the data with:\n",
    "\\[\n",
    "0.02 \\cdot story\\_points \\cdot tech\\_complexity\n",
    "\\]\n",
    "\n",
    "A basic linear regression without this interaction will miss some signal.\n",
    "\n",
    "We can add an engineered feature:\n",
    "- story_points_x_complexity = story_points * tech_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sp_x_complexity\"] = df[\"story_points\"] * df[\"tech_complexity\"]\n",
    "\n",
    "X2 = df.drop(columns=[\"delivery_days\"])\n",
    "y2 = df[\"delivery_days\"]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X2_train, y2_train)\n",
    "\n",
    "report(y2_train, lr2.predict(X2_train), \"Train (with interaction)\")\n",
    "report(y2_test, lr2.predict(X2_test), \"Test  (with interaction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c5940",
   "metadata": {},
   "source": [
    "## 5) Residual visualization (quick preview)\n",
    "\n",
    "Residual = y - y_hat.\n",
    "\n",
    "Good sign:\n",
    "- residuals are randomly scattered around 0\n",
    "\n",
    "Bad sign:\n",
    "- pattern/curve/funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d780ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - pred_test\n",
    "\n",
    "plt.scatter(pred_test, residuals, s=15)\n",
    "plt.axhline(0)\n",
    "plt.title(\"Residuals vs Predictions (baseline multiple LR)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual (actual - predicted)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7c268",
   "metadata": {},
   "source": [
    "## 6) Polynomial regression (why it exists)\n",
    "\n",
    "Sometimes relationship is not linear.\n",
    "Example:\n",
    "- Performance improves with CPU allocation up to a point, then saturates.\n",
    "- Marketing spend increases revenue, but marginal gains reduce after saturation.\n",
    "\n",
    "Polynomial regression:\n",
    "- transforms features into powers: x, x^2, x^3, ...\n",
    "- still uses linear regression under the hood\n",
    "\n",
    "We’ll do a clean 1D example so students can SEE it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = np.linspace(-3, 3, 120)\n",
    "y = 1.2 + 0.5*x - 1.4*(x**2) + 0.3*(x**3) + rng.normal(0, 1.2, size=len(x))\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compare degrees\n",
    "degrees = [1, 2, 3, 8]\n",
    "results = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "        (\"lr\", LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    results.append((d, r2_score(y_test, pred), mean_squared_error(y_test, pred)**0.5))\n",
    "\n",
    "pd.DataFrame(results, columns=[\"degree\", \"R2_test\", \"RMSE_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fits\n",
    "x_plot = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(X_train, y_train, s=15, label=\"train\")\n",
    "plt.scatter(X_test, y_test, s=15, label=\"test\")\n",
    "\n",
    "for d in [1,2,3,8]:\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "        (\"lr\", LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    plt.plot(x_plot, model.predict(x_plot), label=f\"degree={d}\")\n",
    "\n",
    "plt.title(\"Polynomial Regression: Underfit vs Good Fit vs Overfit\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafc01d",
   "metadata": {},
   "source": [
    "## 7) Overfitting intuition (bootcamp talk)\n",
    "\n",
    "- Degree 1: underfit (too simple)\n",
    "- Degree 3: matches true generating process\n",
    "- Degree 8: may start chasing noise\n",
    "\n",
    "Key concept:\n",
    "> More complexity reduces training error, but may increase test error.\n",
    "\n",
    "This is the Bias-Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce1245",
   "metadata": {},
   "source": [
    "## 8) Professional pattern: scaling + polynomial in a pipeline\n",
    "\n",
    "If you do polynomial on real features, values can explode.\n",
    "So scaling helps.\n",
    "\n",
    "We'll show a best-practice pipeline for real work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X2_train, y2_train)\n",
    "report(y2_test, model.predict(X2_test), \"Polynomial(deg=2) on IT dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d20e8d",
   "metadata": {},
   "source": [
    "## 9) Mini Projects (students can do with synthetic or real data)\n",
    "\n",
    "### Project A: Software Sprint Delivery Predictor\n",
    "- Predict delivery days using story points, team size, complexity, dependencies.\n",
    "- Add interaction features.\n",
    "- Compare:\n",
    "  - Multiple LR baseline\n",
    "  - Polynomial with degree 2\n",
    "\n",
    "### Project B: Cloud Cost Estimator\n",
    "- Predict monthly cost using:\n",
    "  - compute_hours\n",
    "  - storage_gb\n",
    "  - network_gb\n",
    "  - region factor\n",
    "- Interpret coefficients.\n",
    "\n",
    "Deliverable:\n",
    "- a notebook report: data → model → metrics → interpretation → recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d459583",
   "metadata": {},
   "source": [
    "## 10) Mini Assignment (must do)\n",
    "\n",
    "1. Add a new feature: `team_size_squared = team_size^2`\n",
    "2. Train model and compare test metrics.\n",
    "3. Explain in 3 lines:\n",
    "   - why such a feature could represent coordination overhead (Brooks’ Law).\n",
    "\n",
    "This is an excellent interview talking point."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
