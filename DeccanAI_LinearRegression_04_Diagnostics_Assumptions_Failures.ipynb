{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ecc8c5",
   "metadata": {},
   "source": [
    "# Linear Regression 04 — Diagnostics, Assumptions & Failures  \n",
    "**Deccan AI School (Premium Bootcamp)** — Working Professionals (IT/Software)\n",
    "\n",
    "**Goal:** Learn production-ready regression:\n",
    "- Assumptions (what must be approximately true)\n",
    "- Diagnostic plots (residuals, QQ plot style intuition)\n",
    "- Outliers and leverage\n",
    "- Multicollinearity and VIF\n",
    "- Heteroscedasticity\n",
    "- Data leakage & drift (engineering traps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5a72e",
   "metadata": {},
   "source": [
    "## 1) Assumptions (talk like a senior data scientist)\n",
    "\n",
    "Linear regression assumes (approximately):\n",
    "1. Linearity: relationship is linear in parameters.\n",
    "2. Independence: observations are independent.\n",
    "3. Homoscedasticity: constant error variance across predictions.\n",
    "4. Normal-ish residuals: mainly for confidence intervals, not prediction accuracy.\n",
    "5. No strong multicollinearity: features not heavily redundant.\n",
    "\n",
    "Reality:\n",
    "- Assumptions are never perfectly true.\n",
    "- Your job is to detect when violation is severe enough to matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b16db",
   "metadata": {},
   "source": [
    "## 2) Build a dataset with issues (so we can diagnose)\n",
    "\n",
    "We intentionally create:\n",
    "- multicollinearity: x2 ~ 2*x1\n",
    "- heteroscedasticity: noise increases with x\n",
    "- some outliers\n",
    "\n",
    "This makes diagnostics meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(10)\n",
    "n = 400\n",
    "\n",
    "x1 = rng.normal(0, 1, n)\n",
    "x2 = 2.0*x1 + rng.normal(0, 0.1, n)        # highly correlated with x1 (collinearity)\n",
    "x3 = rng.normal(0, 1, n)\n",
    "\n",
    "# Heteroscedastic noise: grows with |x1|\n",
    "noise = rng.normal(0, 0.3 + 1.2*np.abs(x1), n)\n",
    "\n",
    "y = 3 + 2.5*x1 - 1.5*x3 + noise\n",
    "\n",
    "# Add a few outliers\n",
    "out_idx = rng.choice(n, 6, replace=False)\n",
    "y[out_idx] += rng.normal(15, 5, size=len(out_idx))\n",
    "\n",
    "df = pd.DataFrame({\"x1\": x1, \"x2\": x2, \"x3\": x3, \"y\": y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8429ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x1\",\"x2\",\"x3\"]]\n",
    "y = df[\"y\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "r2 = r2_score(y_test, pred)\n",
    "rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfca595",
   "metadata": {},
   "source": [
    "## 3) Residual plot: the first diagnostic you should always do\n",
    "\n",
    "If you see:\n",
    "- funnel shape → heteroscedasticity (variance changes with prediction)\n",
    "- curve shape → missing non-linearity\n",
    "- clusters → missing categorical features or segment behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd290b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - pred\n",
    "\n",
    "plt.scatter(pred, residuals, s=15)\n",
    "plt.axhline(0)\n",
    "plt.title(\"Residuals vs Predictions\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual (actual - predicted)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d467bd",
   "metadata": {},
   "source": [
    "## 4) Outlier intuition (bootcamp)\n",
    "\n",
    "Outliers can:\n",
    "- rotate your line (change slope)\n",
    "- inflate error\n",
    "- destabilize coefficients\n",
    "\n",
    "In IT:\n",
    "- Some projects are “special cases” (vendor delay, scope creep, compliance)\n",
    "Treating them like normal cases can hurt.\n",
    "\n",
    "We’ll visualize large residual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_res = np.abs(residuals)\n",
    "top = np.argsort(abs_res)[-10:]\n",
    "\n",
    "plt.scatter(pred, residuals, s=15)\n",
    "plt.scatter(pred[top], residuals.iloc[top], s=60, marker=\"x\")\n",
    "plt.axhline(0)\n",
    "plt.title(\"Residuals with largest-error points highlighted\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1443f5f",
   "metadata": {},
   "source": [
    "## 5) Multicollinearity: when features fight each other\n",
    "\n",
    "If two features contain the same information:\n",
    "- coefficients become unstable\n",
    "- small data changes produce big coefficient changes\n",
    "- interpretation becomes meaningless\n",
    "\n",
    "A quick early hint:\n",
    "- correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076faad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[[\"x1\",\"x2\",\"x3\"]].corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr, aspect=\"auto\")\n",
    "plt.xticks(range(3), [\"x1\",\"x2\",\"x3\"])\n",
    "plt.yticks(range(3), [\"x1\",\"x2\",\"x3\"])\n",
    "plt.title(\"Correlation Heatmap (values shown in table above)\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b10529",
   "metadata": {},
   "source": [
    "## 6) VIF (Variance Inflation Factor)\n",
    "\n",
    "VIF quantifies multicollinearity.\n",
    "\n",
    "Rule-of-thumb:\n",
    "- VIF ~ 1 → no collinearity\n",
    "- VIF > 5 → concerning\n",
    "- VIF > 10 → serious problem\n",
    "\n",
    "We compute VIF manually without external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06171279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def vif(df_features):\n",
    "    X = df_features.values\n",
    "    vifs = []\n",
    "    for i in range(X.shape[1]):\n",
    "        y_i = X[:, i]\n",
    "        X_others = np.delete(X, i, axis=1)\n",
    "        model = LinearRegression().fit(X_others, y_i)\n",
    "        r2 = model.score(X_others, y_i)\n",
    "        vifs.append(1 / (1 - r2))\n",
    "    return pd.DataFrame({\"feature\": df_features.columns, \"VIF\": vifs})\n",
    "\n",
    "vif(df[[\"x1\",\"x2\",\"x3\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f57c69",
   "metadata": {},
   "source": [
    "## 7) What to do when collinearity is high?\n",
    "\n",
    "Practical options:\n",
    "1. Remove one of the correlated features.\n",
    "2. Combine them into one feature (PCA, average, domain logic).\n",
    "3. Use regularization (Ridge helps stabilize).\n",
    "4. Collect more varied data (often best but hardest).\n",
    "\n",
    "We’ll cover Ridge/Lasso as an extension after the linear regression track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e2e86",
   "metadata": {},
   "source": [
    "## 8) Heteroscedasticity (variance changes)\n",
    "\n",
    "If error variance increases with predictions:\n",
    "- standard errors (confidence intervals) become unreliable\n",
    "- you might need:\n",
    "  - transform target (log)\n",
    "  - weighted least squares\n",
    "  - robust regression\n",
    "  - segment models\n",
    "\n",
    "Here, we intentionally built it — so you should see “funnel” behavior in residual plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c856de2",
   "metadata": {},
   "source": [
    "## 9) Leakage & Drift (engineering traps)\n",
    "\n",
    "### Leakage\n",
    "When a feature contains future info:\n",
    "- Example: \"actual_close_date\" used to predict \"delivery_days\"\n",
    "Your model looks perfect in training, then fails in production.\n",
    "\n",
    "### Drift\n",
    "Input distribution changes:\n",
    "- Example: New tech stack introduced, team size policies change.\n",
    "\n",
    "**Senior engineer habit:** Always ask:\n",
    "- “Can this feature exist at prediction time?”\n",
    "- “Will this feature distribution remain stable?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377bbd7",
   "metadata": {},
   "source": [
    "## 10) Mini Assignment (production mindset)\n",
    "\n",
    "1. Remove x2 (highly collinear with x1)\n",
    "2. Retrain and compare:\n",
    "   - RMSE\n",
    "   - coefficient stability (print coefficients)\n",
    "3. Write 5 lines:\n",
    "   - Why removing redundant features can improve interpretability.\n",
    "\n",
    "Deliverable: 1 code cell + 1 markdown cell."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
