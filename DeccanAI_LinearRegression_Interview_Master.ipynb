{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88558c9b",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression — Interview Master Notebook  \n",
    "### Deccan AI School | Premium Bootcamp (Working Professionals)\n",
    "\n",
    "This notebook prepares you for:\n",
    "\n",
    "- Data Scientist interviews  \n",
    "- ML Engineer interviews  \n",
    "- Analytics / BI interviews  \n",
    "- Whiteboard derivations  \n",
    "- Coding rounds  \n",
    "- Case-based discussions  \n",
    "\n",
    "---\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. Read theory sections carefully.\n",
    "2. Solve coding questions without looking at solutions first.\n",
    "3. Practice explaining answers verbally.\n",
    "4. Re-derive formulas by hand at least once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5802a",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 1 — Core Theory Questions (With Answers)\n",
    "\n",
    "---\n",
    "\n",
    "## Q1: What is Linear Regression?\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to predict a continuous target variable by modeling a linear relationship between independent variables (features) and the dependent variable.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ\n",
    "\n",
    "---\n",
    "\n",
    "## Q2: Why do we use Mean Squared Error?\n",
    "\n",
    "- Prevents positive and negative errors from cancelling\n",
    "- Penalizes large errors heavily\n",
    "- Differentiable and convex\n",
    "- Leads to closed-form solution\n",
    "\n",
    "---\n",
    "\n",
    "## Q3: What assumptions does Linear Regression make?\n",
    "\n",
    "1. Linearity  \n",
    "2. Independence of errors  \n",
    "3. Homoscedasticity  \n",
    "4. No multicollinearity  \n",
    "5. Residuals approximately normal (for inference)\n",
    "\n",
    "---\n",
    "\n",
    "## Q4: What happens if XᵀX is not invertible?\n",
    "\n",
    "This happens when:\n",
    "- Features are perfectly collinear\n",
    "- Number of features > number of observations\n",
    "\n",
    "Solutions:\n",
    "- Remove redundant features\n",
    "- Use regularization (Ridge)\n",
    "- Use pseudo-inverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a79c5",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 2 — Mathematical Derivations (Whiteboard Practice)\n",
    "\n",
    "---\n",
    "\n",
    "## Derive the Normal Equation\n",
    "\n",
    "We minimize:\n",
    "\n",
    "J(θ) = (1/n) Σ (y - Xθ)²\n",
    "\n",
    "Take derivative w.r.t θ:\n",
    "\n",
    "∂J/∂θ = (2/n) Xᵀ (Xθ - y)\n",
    "\n",
    "Set derivative to zero:\n",
    "\n",
    "XᵀXθ = Xᵀy\n",
    "\n",
    "θ = (XᵀX)⁻¹ Xᵀy\n",
    "\n",
    "---\n",
    "\n",
    "### Interview Tip:\n",
    "You should be able to derive this in 3–5 minutes on a whiteboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bccc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coding Round Question 1\n",
    "# Implement Linear Regression using Normal Equation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,1],[1,2],[1,3],[1,4]])\n",
    "y = np.array([[2],[3],[4],[5]])\n",
    "\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb25c22",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 3 — Gradient Descent Interview Questions\n",
    "\n",
    "---\n",
    "\n",
    "## Q: When should we prefer Gradient Descent over Normal Equation?\n",
    "\n",
    "- Large datasets\n",
    "- High dimensional data\n",
    "- Distributed systems\n",
    "- Avoid matrix inversion\n",
    "\n",
    "---\n",
    "\n",
    "## Q: What causes Gradient Descent to diverge?\n",
    "\n",
    "- High learning rate\n",
    "- Unscaled features\n",
    "- Poor initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a12348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coding Round Question 2\n",
    "# Implement Batch Gradient Descent\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.01, iterations=1000):\n",
    "    theta = np.zeros((X.shape[1],1))\n",
    "    n = len(y)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        gradient = (2/n) * (X.T @ (X @ theta - y))\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta_gd = gradient_descent(X, y)\n",
    "theta_gd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d418482",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 4 — Conceptual Deep Questions\n",
    "\n",
    "---\n",
    "\n",
    "## Q: Explain Bias-Variance Tradeoff\n",
    "\n",
    "- High Bias → Underfitting\n",
    "- High Variance → Overfitting\n",
    "- Optimal model balances both\n",
    "\n",
    "---\n",
    "\n",
    "## Q: What is Multicollinearity?\n",
    "\n",
    "When independent variables are highly correlated.\n",
    "\n",
    "Effects:\n",
    "- Coefficients unstable\n",
    "- Interpretation unreliable\n",
    "\n",
    "---\n",
    "\n",
    "## Q: What is R²?\n",
    "\n",
    "R² = 1 - (SS_res / SS_total)\n",
    "\n",
    "Measures proportion of variance explained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea47e69",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 5 — Case-Based Interview Questions\n",
    "\n",
    "---\n",
    "\n",
    "## Case 1: Startup Pricing Model\n",
    "\n",
    "You built a pricing prediction model with R² = 0.92 on training but 0.60 on test.\n",
    "\n",
    "Questions:\n",
    "- What happened?\n",
    "- How would you fix it?\n",
    "- What diagnostics would you run?\n",
    "\n",
    "Expected Discussion:\n",
    "- Overfitting\n",
    "- Cross-validation\n",
    "- Feature engineering\n",
    "- Regularization\n",
    "\n",
    "---\n",
    "\n",
    "## Case 2: IT Project Estimation Model Fails in Production\n",
    "\n",
    "Possible reasons:\n",
    "- Data drift\n",
    "- Leakage during training\n",
    "- Change in business process\n",
    "- Unseen feature distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f35f6f",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 6 — Rapid Fire Interview Questions (Answer Verbally)\n",
    "\n",
    "1. Why square error instead of absolute error?\n",
    "2. What if residuals show funnel shape?\n",
    "3. Difference between Ridge and Lasso?\n",
    "4. What is Adjusted R²?\n",
    "5. Can Linear Regression handle categorical features?\n",
    "6. What happens if we add irrelevant features?\n",
    "7. Explain feature scaling importance.\n",
    "8. What is the geometric interpretation of OLS?\n",
    "9. How does regularization modify cost function?\n",
    "10. When is Linear Regression not appropriate?\n",
    "\n",
    "Practice answering each in under 60 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ca282",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 7 — Resume Talking Points\n",
    "\n",
    "You should be able to say:\n",
    "\n",
    "- “Implemented Linear Regression from scratch including gradient descent and normal equation.”\n",
    "- “Performed residual diagnostics and multicollinearity analysis.”\n",
    "- “Engineered interaction features to improve generalization.”\n",
    "- “Validated model using RMSE, MAE and R² metrics.”\n",
    "\n",
    "---\n",
    "\n",
    "# Final Advice\n",
    "\n",
    "In interviews:\n",
    "\n",
    "1. Think aloud.\n",
    "2. Write equations clearly.\n",
    "3. Explain intuition, not just formula.\n",
    "4. Connect to business context.\n",
    "5. Mention limitations.\n",
    "\n",
    "---\n",
    "\n",
    "End of Interview Master Notebook.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
