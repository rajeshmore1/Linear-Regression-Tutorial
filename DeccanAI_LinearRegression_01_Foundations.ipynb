{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca65d7a8",
   "metadata": {},
   "source": [
    "# Linear Regression 01 — Foundations & Mathematical Intuition  \n",
    "**Deccan AI School (Premium Bootcamp)** — Working Professionals (IT/Software)  \n",
    "**Goal:** Understand *why* linear regression works, not just how to call a library.\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "- Read the markdown carefully (it is written as if your instructor is speaking).\n",
    "- Run code cells top to bottom.\n",
    "- Pause at \"Stop & Think\" prompts — these are interview-style checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will learn\n",
    "1. Regression vs Classification (in practical terms)\n",
    "2. What a \"line of best fit\" really means (geometry + intuition)\n",
    "3. The hypothesis function and why we use MSE\n",
    "4. Residuals: the core object you should think about\n",
    "5. A gentle bridge to statistics: mean/variance/covariance/correlation\n",
    "6. What the model can and cannot do in real systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9aecce",
   "metadata": {},
   "source": [
    "## 1) Regression in the real world (Manager mindset)\n",
    "\n",
    "If your manager asks:\n",
    "- “How much will it cost if we add 2 engineers?”\n",
    "- “How will revenue change if we increase marketing spend?”\n",
    "- “What’s the expected delivery time if we change the route length?”\n",
    "… they are implicitly asking for **a function that maps inputs → a number**.\n",
    "\n",
    "That is **regression**.\n",
    "\n",
    "### Why Linear Regression is still used in 2026\n",
    "Even in the era of deep learning:\n",
    "- It’s a strong baseline (fast + explainable).\n",
    "- It gives interpretable coefficients (feature impact).\n",
    "- It is cheap to train and deploy.\n",
    "- It’s often “good enough” and safer to ship.\n",
    "\n",
    "> In many orgs: you start with Linear Regression, then you prove why you need something more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b012ad",
   "metadata": {},
   "source": [
    "## 2) Regression vs Classification (Software analogy)\n",
    "\n",
    "Think like a backend engineer:\n",
    "\n",
    "- **Classification** is like choosing a label / route:\n",
    "  - `/approve` vs `/reject`\n",
    "  - `spam` vs `not spam`\n",
    "\n",
    "- **Regression** is like returning a numeric response:\n",
    "  - `ETA = 24 minutes`\n",
    "  - `price = ₹ 1,25,000`\n",
    "  - `CPU utilization = 73%`\n",
    "\n",
    "So regression outputs a **continuous** value.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) The simplest regression model\n",
    "\n",
    "We assume a relationship:\n",
    "\n",
    "\\[\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\theta_0\\) = intercept (baseline when x=0)\n",
    "- \\(\\theta_1\\) = slope (change in y for 1 unit change in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used across the notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nicer plots (no special styling, just grid)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b95a84",
   "metadata": {},
   "source": [
    "## 4) A toy dataset (IT Salary example)\n",
    "\n",
    "We’ll start with a tiny dataset.\n",
    "- `x` = years of experience\n",
    "- `y` = salary in LPA (rough, simplified)\n",
    "\n",
    "This is just to build intuition first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb556dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=float)\n",
    "y = np.array([3.5, 4.2, 5.1, 6.0, 6.8, 7.6, 8.1, 9.0, 9.7, 10.4, 11.2], dtype=float)\n",
    "\n",
    "df = pd.DataFrame({\"years_exp\": x, \"salary_lpa\": y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f60542",
   "metadata": {},
   "source": [
    "## 5) Visual intuition: “What are we trying to do?”\n",
    "\n",
    "We want a line that is “as close as possible” to the points.\n",
    "\n",
    "But close in what sense?\n",
    "\n",
    "We need a definition of \"best\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5123261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary (LPA)\")\n",
    "plt.title(\"Data: Experience vs Salary\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b0912",
   "metadata": {},
   "source": [
    "## 6) Residuals: the most important concept\n",
    "\n",
    "For any point:\n",
    "- prediction = \\(\\hat{y}\\)\n",
    "- actual = \\(y\\)\n",
    "\n",
    "Residual:\n",
    "\\[\n",
    "r = y - \\hat{y}\n",
    "\\]\n",
    "\n",
    "A good model has residuals that are:\n",
    "- small on average\n",
    "- not systematically patterned (we'll study this later)\n",
    "\n",
    "**Stop & Think (Interview):**  \n",
    "If residuals show a curve pattern, what does it indicate?\n",
    "- Hint: your model is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759c9b3",
   "metadata": {},
   "source": [
    "## 7) Why Mean Squared Error (MSE)?\n",
    "\n",
    "Cost function:\n",
    "\\[\n",
    "J(\\theta_0,\\theta_1) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "Why square?\n",
    "- Negative + positive errors shouldn’t cancel out.\n",
    "- Big errors should be punished more.\n",
    "- Squared function gives a smooth, convex surface (easy to optimize).\n",
    "\n",
    "**Practical note:** MSE is not the only choice, but it is the standard baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0b1d5",
   "metadata": {},
   "source": [
    "## 8) Try a random line and compute the MSE (manual intuition)\n",
    "\n",
    "We’ll pick a slope and intercept and see how bad it is.\n",
    "\n",
    "This is how you develop intuition: *try → measure → improve*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10237842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_line(x, theta0, theta1):\n",
    "    return theta0 + theta1 * x\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "theta0, theta1 = 3.0, 0.8\n",
    "y_hat = predict_line(x, theta0, theta1)\n",
    "print(\"MSE:\", mse(y, y_hat))\n",
    "\n",
    "plt.scatter(x, y, label=\"Actual\")\n",
    "plt.plot(x, y_hat, label=f\"Pred: theta0={theta0}, theta1={theta1}\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary (LPA)\")\n",
    "plt.title(\"A random line + its fit\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172489a5",
   "metadata": {},
   "source": [
    "## 9) Geometry: “Best fit” is minimizing total squared vertical distance\n",
    "\n",
    "Important: in standard linear regression we minimize vertical distances (errors in y),\n",
    "assuming x is measured without error (or error is negligible).\n",
    "\n",
    "In some fields (physics), they also consider errors in x (total least squares),\n",
    "but for business prediction, standard least squares is typical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82dbac",
   "metadata": {},
   "source": [
    "## 10) Statistics bridge (light, but meaningful)\n",
    "\n",
    "Two key ideas:\n",
    "\n",
    "### Variance  \n",
    "How much a variable spreads out around its mean.\n",
    "\n",
    "### Covariance  \n",
    "How much two variables change together.\n",
    "- Positive covariance → when x increases, y tends to increase\n",
    "- Negative covariance → when x increases, y tends to decrease\n",
    "\n",
    "Correlation is normalized covariance:\n",
    "\\[\n",
    "\\rho = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}\n",
    "\\]\n",
    "\n",
    "Correlation helps you anticipate the sign of slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794abc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation for intuition\n",
    "corr = np.corrcoef(x, y)[0, 1]\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa272665",
   "metadata": {},
   "source": [
    "## 11) Closed-form solution (preview)\n",
    "\n",
    "In later notebooks we’ll derive it fully, but here’s the key fact:\n",
    "\n",
    "There is a direct formula for the best parameters:\n",
    "\\[\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "\\]\n",
    "\n",
    "This is called:\n",
    "- Normal Equation\n",
    "- Ordinary Least Squares (OLS) solution\n",
    "\n",
    "We’ll implement it next, then later derive it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build design matrix with bias term\n",
    "X = np.c_[np.ones_like(x), x]  # shape (n, 2)\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "\n",
    "theta0_best, theta1_best = theta\n",
    "theta0_best, theta1_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_best = X @ theta\n",
    "\n",
    "print(\"Best-fit MSE:\", mse(y, y_hat_best))\n",
    "\n",
    "plt.scatter(x, y, label=\"Actual\")\n",
    "plt.plot(x, y_hat_best, label=\"Best-fit line (OLS)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary (LPA)\")\n",
    "plt.title(\"OLS Best Fit\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b014ea3",
   "metadata": {},
   "source": [
    "## 12) Interpret coefficients like a professional\n",
    "\n",
    "If \\(\\theta_1 = 0.76\\), you should say:\n",
    "\n",
    "> “On average, an additional 1 year of experience increases salary by ~0.76 LPA, **assuming other factors remain constant**.”\n",
    "\n",
    "That last part (“other factors remain constant”) becomes crucial in **multiple regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065fead",
   "metadata": {},
   "source": [
    "## 13) Common misconceptions (Bootcamp warnings)\n",
    "\n",
    "1. **High correlation does NOT mean causation**\n",
    "   - Salary and experience correlate, but other variables exist.\n",
    "\n",
    "2. **Linear regression does NOT guarantee linear reality**\n",
    "   - It assumes a linear approximation.\n",
    "\n",
    "3. **Good training fit is not enough**\n",
    "   - You need generalization (test performance).\n",
    "\n",
    "4. **Outliers can hijack the line**\n",
    "   - One weird point can rotate your slope.\n",
    "\n",
    "We’ll build diagnostics later to detect these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392a476",
   "metadata": {},
   "source": [
    "## 14) Mini Assignment (Do it now)\n",
    "\n",
    "1. Create a new dataset where salary increases faster after 6 years (non-linear).\n",
    "2. Fit a linear line and plot residuals.\n",
    "3. Write 3 lines explaining why linear regression struggles there.\n",
    "\n",
    "**Deliverable:** One plot + one markdown explanation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
